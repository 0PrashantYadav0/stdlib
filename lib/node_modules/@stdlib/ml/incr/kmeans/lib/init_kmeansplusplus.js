/**
* @license Apache-2.0
*
* Copyright (c) 2018 The Stdlib Authors.
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*    http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

'use strict';

// MODULES //

var randint = require( '@stdlib/random/base/discrete-uniform' ).factory;
var randu = require( '@stdlib/random/base/randu' ).factory;
var norm = require( './normalize.js' );
var standardize = require( './standardize.js' );
var dot = require( './dot_product.js' );
var euclidean = require( './euclidean.js' );


// MAIN //

/**
* Initializes centroids by performing the k-means++ initialization procedure.
*
* ## Methods
*
* The k-means++ algorithm for choosing initial centroids is as follows:
*
* 1.  Select a data point uniformly at random from a data set \\( X \\). This data point is first centroid and denoted \\( c_0 \\).
*
* 2.  Compute the distance from each data point to \\( c_0 \\). Denote the distance between \\( c_j \\) and data point \\( m \\) as \\( d(x_m, c_j) \\).
*
* 3.  Select the next centroid, \\( c_1 \\), at random from \\( X \\) with probability
*
*     ```tex
*     \frac{d^2(x_m, c_0)}{\sum_{j=0}^{n-1} d^2(x_j, c_0)}
*     ```
*
*     where \\( n \\) is the number of data points.
*
* 4.  To choose centroid \\( j \\),
*
*     a.   Compute the distances from each data point to each centroid and assign each data point to its closest centroid.
*
*     b.   For \\( i = 0,\ldots,n-1 \\) and \\( p = 0,\ldots,j-2 \\), select centroid \\( j \\) at random from \\( X \\) with probability
*
*          ```tex
*          \frac{d^2(x_i, c_p)}{\sum_{\{h; x_h \exits C_p\}} d^2(x_h, c_p)}
*          ```
*
*          where \\( C_p \\) is the set of all data points closest to centroid \\( c_p \\) and \\( x_i \\) belongs to \\( c_p \\).
*
*          Stated more plainly, select each subsequent centroid with a probability proportional to the distance from the centroid to the closest centroid already chosen.
*
* 5.  Repeat step `4` until \\( k \\) centroids have been chosen.
*
* @private
* @param {ndarray} out - output centroids `kxd` matrix
* @param {ndarray} buffer - data buffer
* @param {string} metric - distance metric
* @param {boolean} normalize - boolean indicating whether to normalize data vectors (only relevant for non-Euclidean distance metrics)
* @param {PositiveInteger} trials - number of potential centroids per iteration
* @param {*} seed - PRNG seed
* @returns {ndarray} centroids
*/
function kmeansplusplus( out, buffer, metric, normalize, trials, seed ) {
	var centroids; // array of indices
	var randi;
	var ndims;
	var rand;
	var npts;
	var sum;
	var d2;
	var c;
	var d;
	var k;
	var r;
	var i;
	var j;

	k = out.shape[ 0 ];
	ndims = out.shape[ 1 ];
	npts = buffer.shape[ 0 ];

	// Create seeded PRNGs:
	randi = randint({
		'seed': seed
	});
	rand = randu({
		'seed': randi()
	});

	// 0. If required by the metric, normalize the data vectors along the dimensions (TODO: unsure whether normalization is strictly necessary here, but, out of precaution, normalizing in order to maintain relationship with Euclidean distance)...
	if ( normalize ) {
		if ( metric === 'cosine' ) {
			buffer = norm( buffer );
		} else if ( metric === 'correlation' ) {
			buffer = standardize( buffer );
		}
	}
	// 1. Select a data point at random for the first centroid...
	c = randi( 0, npts );
	if ( k === 1 ) {
		// For the trivial case of one centroid, we are done...
		for ( j = 0; j < ndims; j++ ) {
			out.set( 0, j, buffer.get( c, j ) );
		}
		return out;
	}
	centroids = [ c ];

	// 2. Compute the distances from each data point to the first centroid...
	sum = 0.0; // used in step 3
	d2 = []; // squared distances used in step 3
	if ( metric === 'cosine' || metric === 'correlation' ) {
		for ( i = 0; i < npts; i++ ) {
			d = 1.0 - dot( buffer, i, buffer, c );
			d *= d;
			d2.push( d );
			sum += d;
		}
	} else {
		for ( i = 0; i < npts; i++ ) {
			d = euclidean( buffer, i, buffer, c );
			d *= d;
			d2.push( d );
			sum += d;
		}
	}

	// 3. Transform the distances to cumulative probabilities and select the next centroid...
	while ( centroids.length < 2 ) { // Use rejection sampling to handle edge case where the total cumulative probability does not equal unity and is less than `r` due to accumulated floating-point errors
		r = rand();

		// Note: the following should never choose `c_0` (why? because its squared distance is `0`, which means it will correspond to a cumulative probability of `0`, which `r` is never less than, or will correspond to a cumulative probability equal to the previous cumulative probability, thus leading to the equivalent of a no-op iteration)
		d2[ 0 ] /= sum;
		if ( r < d2[ 0 ] ) {
			c = 0;
		} else {
			for ( i = 1; i < npts; i++ ) {
				d2[ i ] = d2[ i-1 ] + ( d2[ i ] / sum );
				if ( r < d2[ i ] ) {
					c = i;
					break;
				}
			}
		}
		centroids.push( c );
	}

	// 4-5. For each data point, find the closest centroid, compute the distance to the closest centroid, and, based on that distance, assign a probability to the data point to be chosen as centroid `c_j`...
	while ( centroids.length < k ) {
		// TODO: implement
	}

	// 6. Set centroid data...
	for ( i = 0; i < k; i++ ) {
		c = centroids[ i ];
		for ( j = 0; j < ndims; j++ ) {
			out.set( i, j, buffer.get( c, j ) );
		}
	}
	return out;
}


// EXPORTS //

module.exports = kmeansplusplus;
