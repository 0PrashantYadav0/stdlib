/**
* @license Apache-2.0
*
* Copyright (c) 2018 The Stdlib Authors.
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*    http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

'use strict';

// MODULES //

var randint = require( '@stdlib/random/base/discrete-uniform' ).factory;
var norm = require( './normalize.js' );
var standardize = require( './standardize.js' );


// MAIN //

/**
* Initializes centroids by performing the k-means++ initialization procedure.
*
* ## Methods
*
* The k-means++ algorithm for choosing initial centroids is as follows:
*
* 1.  Select a data point uniformly at random from a data set \\( X \\). This data point is first centroid and denoted \\( c_0 \\).
*
* 2.  Compute the distance from each data point to \\( c_0 \\). Denote the distance between \\( c_j \\) and data point \\( m \\) as \\( d(x_m, c_j) \\).
*
* 3.  Select the next centroid, \\( c_1 \\), at random from \\( X \\) with probability
*
*     ```tex
*     \frac{d^2(x_m, c_0)}{\sum_{j=0}^{n-1} d^2(x_j, c_0)}
*     ```
*
*     where \\( n \\) is the number of data points.
*
* 4.  To choose centroid \\( j \\),
*
*     a.   Compute the distances from each data point to each centroid and assign each data point to its closest centroid.
*
*     b.   For \\( i = 0,\ldots,n-1 \\) and \\( p = 0,\ldots,j-2 \\), select centroid \\( j \\) at random from \\( X \\) with probability
*
*          ```tex
*          \frac{d^2(x_i, c_p)}{\sum_{\{h; x_h \exits C_p\}} d^2(x_h, c_p)}
*          ```
*
*          where \\( C_p \\) is the set of all data points closest to centroid \\( c_p \\) and \\( x_i \\) belongs to \\( c_p \\).
*
*          Stated more plainly, select each subsequent centroid with a probability proportional to the distance from the centroid to the closest centroid already chosen.
*
* 5.  Repeat step `4` until \\( k \\) centroids have been chosen.
*
* @private
* @param {ndarray} out - output centroids `kxd` matrix
* @param {ndarray} buffer - data buffer
* @param {string} metric - distance metric
* @param {boolean} normalize - boolean indicating whether to normalize data vectors (only relevant for non-Euclidean distance metrics)
* @param {PositiveInteger} trials - number of potential centroids per iteration
* @param {*} seed - PRNG seed
* @returns {ndarray} centroids
*/
function kmeansplusplus( out, buffer, metric, normalize, trials, seed ) {
	var centroids; // array of indices
	var randi;
	var ndims;
	var npts;
	var c;
	var k;
	var i;
	var j;

	k = out.shape[ 0 ];
	ndims = out.shape[ 1 ];
	npts = buffer.shape[ 0 ];

	// Create a seeded PRNG:
	randi = randint({
		'seed': seed
	});

	// 0. If required by the metric, normalize the data vectors along the dimensions (TODO: unsure whether normalization is strictly necessary here, but, out of precaution, normalizing in order to maintain relationship with Euclidean distance)...
	if ( normalize ) {
		if ( metric === 'cosine' ) {
			buffer = norm( buffer );
		} else if ( metric === 'correlation' ) {
			buffer = standardize( buffer );
		}
	}
	// 1. Select a data point at random for the first centroid:
	centroids = [ randi( 0, npts ) ];

	// 6. Set centroid data...
	for ( i = 0; i < k; i++ ) {
		c = centroids[ i ];
		for ( j = 0; j < ndims; j++ ) {
			out.set( i, j, buffer.get( c, j ) );
		}
	}
	return out;
}


// EXPORTS //

module.exports = kmeansplusplus;
