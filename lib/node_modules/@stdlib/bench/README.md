# Benchmark

> Benchmark harness.


<!-- Section to include introductory text. Make sure to keep an empty line after the intro `section` element and another before the `/section` close. -->

<section class="intro">

</section>

<!-- /.intro -->

<!-- Package usage documentation. -->

<section class="usage">

## Usage

``` javascript
var bench = require( '@stdlib/bench' );
```

#### bench( name\[, options\]\[, benchmark\] )

Queues a `benchmark` to be run during a subsequent turn of the event loop.

``` javascript
bench( 'Math.sin', function benchmark( b ) {
    var x;
    var i;
    b.comment( 'Running benchmarks...' );
    b.tic();
    for ( i = 0; i < b.iterations; i++ ) {
        x = Math.sin( Math.random() );
        if ( x !== x ) {
            b.fail( 'should not return NaN' );
        }
    }
    b.toc();
    if ( x !== x ) {
        b.fail( 'should not return NaN' );
    }
    b.comment( 'Finished running benchmarks.' );
    b.end();
});
```

A `benchmark` function has the following signature:

``` javascript
function benchmark( b ) {
    // Benchmark code...
}
```

where `b` is a `Benchmark` instance. Synchronous benchmarks should, at minimum, have the following structure:

``` javascript
function benchmark( b ) {
    var x;
    var i;

    // [1] Start timing:
    b.tic();

    // [2] Loop containing code to time...
    for ( i = 0; i < b.iterations; i++ ) {
        // [3] Code to time...

        // [4] A conditional verifying results to prevent certain compiler optimizations:
        if ( x !== x ) {
            b.fail( 'something went wrong!' );
        }
    }
    // [5] Stop timing:
    b.toc();

    // [6] Another conditional verifying results to prevent certain compiler optimizations:
    if ( x !== x ) {
        b.fail( 'something went wrong!' );
    }
    // [7] End the benchmark:
    b.end();
}
```

Asynchronous benchmarks should a structure similar to the following:

``` javascript
function benchmark( b ) {
    var i = 0;

    // [1] Start timing:
    b.tic();

    // [2] Asynchronous code to time:
    setTimeout( next, 0 );

    function next( error ) {
        if ( error ) {
            return b.fail( error.message );
        }
        i += 1;

        // [3] Exit condition:
        if ( i < b.iterations ) {
            return setTimeout( next, 0 );
        }
        // [4] Stop timing:
        b.toc();

        // [5] End the benchmark:
        b.end();
    }
}
```

For both synchronous and asynchronous benchmarks, calling `b.end()` is __mandatory__, as failing to do so will cause the harness to hang. For example, the following benchmark will __never__ complete.

``` javascript
function benchmark( b ) {
    b.tic();
    b.toc();
}
```

__Avoid__ making assertions within timed code, as doing so will __significantly__ affect raw performance numbers. For example, __avoid__ the following:

``` javascript
function benchmark( b ) {
    var x;
    var i;

    b.tic();
    for ( i = 0; i < b.iterations; i++ ) {
        x = Math.sin( Math.random() );
        b.equal( x, x, 'does not return NaN' ); // Avoid doing this!
    }
    b.toc();
    b.equal( x, x, 'does not return NaN' );     // This is fine.
    b.end();
}
```

Additionally, ensure that all setup code executes __before__ calling `b.tic()` and that all cleanup code executes __after__ calling `b.toc()`. For example, __avoid__ the following:

``` javascript
function benchmark( b ) {
    var x;
    var y;
    var i;

    // Start timing:
    b.tic();

    // Setup code:
    x = new Array( b.iterations );                  // Should be before b.tic()!
    for ( i = 0; i < b.iterations; i++ ) {
        x[ i ] = Math.random();
    }
    // Code to be timed...
    for ( i = 0; i < b.iterations; i++ ) {
        y = Math.sin( x[ i ] );
        if ( y !== y ) {
            b.fail( 'should not return NaN' );
        }
    }
    // Verify results:
    b.equal( x, x, 'does not return NaN' );         // Should be after b.toc()!

    // Stop timing:
    b.toc();

    b.end();
}
```

The function accepts the following `options`:

* __iterations__: number of iterations. If `null`, the number of iterations is determined by trying successive powers of `10` until the total time is at least `0.1` seconds. Default: `null`.
* __repeats__: number of repeats. Default: `3`.
* __timeout__: number of milliseconds before a benchmark is considered failed. Exceeding a timeout does __not__, however, end the benchmark. Ending a long running benchmark requires manual intervention. Default: `300000` (5 minutes).
* __skip__: `boolean` indicating whether to skip a benchmark.

TODO: examples using options


#### bench.onFinish( clbk )

TODO: document


#### bench.createStream( \[options\] )

TODO: document


#### bench.createHarness( \[options\]\[, clbk\] )

Creates a benchmark harness with a new pending stack and state.

TODO: document

The method accepts the following `options`:

* __autoclose__: `boolean` indicating whether to automatically close a harness after running all benchmarks.


##### harness.createStream( \[options\] )

TODO: document


##### harness.close()

TODO: document


__Note__: a running benchmark is allowed to finish before closing a harness. Any pending benchmarks are cleared from the harness stack.


##### harness.exit()

TODO: document

__Note__: all currently running and pending benchmarks will generate __failing__ assertions.


##### harness.exitCode

TODO: document


---

### Benchmark

A `Benchmark` instance has the following properties and methods...

#### b.name

__Read-only__ property whose value is the benchmark `name`.

``` javascript
var name = b.name;
// returns <string>
```

#### b.iterations

__Read-only__ property whose value is the number of iterations.

``` javascript
var iter = b.iterations;
// returns <number>
```

#### b.tic()

Starts a benchmark timer. In order to benchmark code, this method __must always__ be called within a `benchmark` function.

``` javascript
function benchmark( b ) {
    var x;
    var i;
    
    // Start a timer:
    b.tic();

    for ( i = 0; i < b.iterations; i++ ) {
        x = Math.sin( Math.random() );
        if ( x !== x ) {
            b.fail( 'should not return NaN' );
        }
    }
    b.toc();
    if ( x !== x ) {
        b.fail( 'should not return NaN' );
    }
    b.end();
}
```

#### b.toc()

Stops a benchmark timer. In order to benchmark code, this method __must always__ be called within a `benchmark` function.

``` javascript
function benchmark( b ) {
    var x;
    var i;
    
    b.tic();
    for ( i = 0; i < b.iterations; i++ ) {
        x = Math.sin( Math.random() );
        if ( x !== x ) {
            b.fail( 'should not return NaN' );
        }
    }
    // Stop a timer:
    b.toc();

    if ( x !== x ) {
        b.fail( 'should not return NaN' );
    }
    b.end();
}
```

#### b.end()

Explicitly ends a benchmark. In order to benchmark code, this method __must always__ be called within a `benchmark` function.

``` javascript
function benchmark( b ) {
    var x;
    var i;
    
    b.tic();
    for ( i = 0; i < b.iterations; i++ ) {
        x = Math.sin( Math.random() );
        if ( x !== x ) {
            b.fail( 'should not return NaN' );
        }
    }
    b.toc();

    if ( x !== x ) {
        b.fail( 'should not return NaN' );
    }

    // Explicitly end the benchmark:
    b.end();
}
```

__Warning__: no assertions should follow a call to `b.end()`. Including assertions after `b.end()` may result in interleaved [TAP][tap] output or an output stream closing before a benchmark executes pending assertions.


#### b.comment( msg )

Writes a message without breaking [TAP][tap] output.

``` javascript
b.comment( 'Running benchmark...' );
```

``` text
TODO
```

#### b.skip( value, msg )

Generates an assertion which will be skipped.

``` javascript
b.skip( false, 'Skipping this assertion.' );
```

``` text
TODO
```

#### b.todo( value, msg )

Generates an assertion which should be implemented.

``` javascript
b.todo( false, 'need to implement this feature' );
```

``` text
TODO
```

#### b.fail( msg )

Generates a failing assertion.

``` javascript
b.fail( 'something went wrong' );
```

``` text
TODO
```

#### b.pass( msg )

Generates a passing assertion.

``` javascript
b.pass( 'everything is fine' );
```

``` text
TODO
```

#### b.ok( value\[, msg\] )

Asserts that a `value` is truthy.

``` javascript
b.ok( [] );
```

``` text
TODO
```

To override the default message, provide a `msg` argument.

``` javascript
b.ok( null, 'should not be falsey' );
```

``` text
TODO
```

#### b.notOk( value\[, msg\] )

Asserts that a `value` is falsey.

``` javascript
b.notOk( null );
```

``` text
TODO
```

To override the default message, provide a `msg` argument.

``` javascript
b.notOk( {}, 'should not be truthy' );
```

``` text
TODO
```

#### b.equal( actual, expected\[, msg\] )

Asserts that `actual` is __strictly__ equal to `expected`.

``` javascript
var expected = [];
var actual = expected;

b.equal( actual, expected );
```

``` text
TODO
```

To override the default message, provide a `msg` argument.

``` javascript
var expected = [];
var actual = expected;

b.equal( actual, expected, 'should be the same reference' );
```

``` text
TODO
```

#### b.notEqual( actual, expected\[, msg\] )

Asserts that `actual` is not __strictly__ equal to `expected`.

``` javascript
var expected = [];
var actual = [];

b.notEqual( actual, expected );
```

``` text
TODO
```

To override the default message, provide a `msg` argument.

``` javascript
var expected = [];
var actual = [];

b.notEqual( actual, expected, 'should be distinct references' );
```

``` text
TODO
```

#### b.deepEqual( actual, expected\[, msg\] )

Asserts that `actual` is __deeply__ equal to `expected`.

``` javascript
var expected = {
    'a': 'b'
};
var actual = {
    'a': 'b'
};

b.deepEqual( actual, expected );
```

``` text
TODO
```

To override the default message, provide a `msg` argument.

``` javascript
var expected = {
    'a': 'b'
};
var actual = {
    'a': 'b'
};

b.deepEqual( actual, expected, 'should be a clone' );
```

``` text
TODO
```

#### b.notDeepEqual( actual, expected\[, msg\] )

Asserts that `actual` is not __deeply__ equal to `expected`.

``` javascript
var expected = {
    'a': 'b'
};
var actual = {
    'a': 'c'
};

b.notDeepEqual( actual, expected );
```

``` text
TODO
```

To override the default message, provide a `msg` argument.

``` javascript
var expected = {
    'a': 'b'
};
var actual = {
    'a': 'c'
};

b.notDeepEqual( actual, expected, 'should contain different property values' );
```

``` text
TODO
```

</section>

<!-- /.usage -->

<!-- Package usage notes. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->

---

<section class="notes">

## Notes

* All benchmark functions execute __serially__ in separate turns of the event loop.
* All benchmark functions should be added during the __same__ turn of the event loop. Otherwise, you will likely encounter raise conditions where a benchmark executes and finishes causing a harness to close __before__ subsequent benchmarks register.
* Similarly, create results streams __before__ adding benchmarks to the harness.
* All benchmarks are pretested. If a benchmark generates failing assertions or fails to call `b.tic()` and/or `b.toc()` during pretests (even if due to an intermittent failure), a benchmark is __only__ run once (i.e., `options.repeats` is ignored). Similarly, if `options.iterations` is `null` and a benchmark fails during iteration number determination, a benchmark is __only__ run once and for one iteration. Accordingly, if a benchmark does not run an expected number of repetitions and/or iterations, this behavior is likely attributable to a benchmark failure during pretesting.
* All benchmarks must have a `name`. If a `name` is not provided, the harness will throw an `Error`.
* While not required, all benchmarks should have a __unique__ `name`. Unique names ensure easier identification and assignment of benchmark results.
* Uncaught exceptions in benchmark functions are __not__ intercepted and will cause the harness to error.
* If any one of `b.tic()`, `b.toc()`, or `b.end()` is called more than once within a benchmark, the benchmark will __fail__.
* __Always__ verify results. Doing so prevents the compiler from performing dead code elimination and other optimization techniques, which would render timing results meaningless.
* While many benchmark frameworks calculate various statistics over raw timing results (e.g., mean and standard deviation), do __not__ do this. Instead, consider the fastest time an approximate lower bound for how fast an environment can execute benchmark code. Slower times are more likely attributable to other processes interfering with timing accuracy rather than attributable to variability in JavaScript's speed. In which case, the minimum time is most likely the only result of interest. When considering all raw timing results, apply common sense rather than statistics.

</section>

<!-- /.notes -->

<!-- Package usage examples. -->

---

<section class="examples">

## Examples

``` javascript
var bench = require( '@stdlib/bench' );
```

</section>

<!-- /.examples -->

<!-- Section for describing a command-line interface. -->

---

<section class="cli">

## CLI

<!-- CLI usage documentation. -->

<section class="usage">

### Usage

``` bash
Usage: bench [options] TODO

Options:

  -h,    --help                Print this message.
  -V,    --version             Print the package version.
```

</section>

<!-- /.usage -->

<!-- CLI usage notes. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->

<section class="notes">

### Notes

TODO

</section>

<!-- /.notes -->

<!-- CLI usage examples. -->

<section class="examples">

### Examples

``` bash
$ bench TODO
```

will generate [TAP][tap] output similar to the following

``` text
TODO
```

</section>

<!-- /.examples -->

</section>

<!-- /.cli -->

<!-- Section to include cited references. If references are included, add a horizontal rule *before* the section. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->

---

<section class="references">

## References

* Chen, Jiahao, and Jarrett Revels. 2016. "Robust benchmarking in noisy environments." *CoRR* abs/1608.04295 (August). <http://arxiv.org/abs/1608.04295>.

</section>

<!-- /.references -->

<!-- Section for all links. Make sure to keep an empty line after the `section` element and another before the `/section` close. -->

<section class="links">

[tap]: https://testanything.org/tap-specification.html

</section>

<!-- /.links -->
