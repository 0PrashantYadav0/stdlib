'use strict';

// MODULES //

var isArray = require( '@stdlib/utils/is-array' );
var sqrt = require( '@stdlib/math/base/special/sqrt' );
var WeightVector = require( './weight_vector.js' );
var validate = require( './validate.js' );


// FUNCTIONS //

var max = Math.max;


// CONSTANTS //

var MIN_SCALING_FACTOR = 0.0000001;


// ONLINE SGD REGRESSION //

/**
* Online learning for regression using stochastic gradient descent (SGD).
*
* #### Method
*
* The sub-gradient of the loss function is estimated for each datum
* and the regression model is updated incrementally, with a decreasing learning rate
* and regularization of the feature weights based on L2 regularization.
*
* Reference:
* * Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011).
*   Pegasos: Primal estimated sub-gradient solver for SVM.
*   Mathematical Programming, 127(1), 3â€“30. doi:10.1007/s10107-010-0420-4
*
* @constructor
* @param {Object} [options] - options object
* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter
* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate
* @param {PositiveNumber} [options.lambda=1e-3] - regularization parameter
* @param {string} [options.learningRate='pegasos'] - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.
* @param {string} [options.loss='epsilonInsensitive'] - string denoting the loss function to use. Can be `leastSquares`, `epsilonInsensitive` or `huber`.
* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept
* @throws {TypeError} must provide valid options
* @returns {OnlineLearner} class instance
*
* @example
* var OnlineSGDRegression = require( '@stdlib/streams/math/ml/online-sgd-regression' );
*
* var model = OnlineSGDRegression({
* 	'intercept': true
* 	'lambda': 1e-5
* });
*
* // Update model as observations come in:
* var y = 3.5;
* var x = [ 2.3, 1.0, 5.0 ];
* model.update( y, x );
*
* // Predict new observation:
* var yHat = model.predict( x );
*
* // Retrieve coefficients:
* var coefs = model.coefs;
*/
function OnlineSGDRegression( options ) {
	var self = this;

	/**
	* Initialize learner.
	*
	* @private
	* @param {Object} [options] - options object
	* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter
	* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate
	* @param {PositiveNumber} [options.lambda=1e-3] - regularization parameter
	* @param {string} [options.learningRate='pegasos'] - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.
	* @param {string} [options.loss='epsilonInsensitive'] - string denoting the loss function to use. Can be `leastSquares`, `epsilonInsensitive` or `huber`.
	* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept
	* @throws {TypeError} must provide valid options
	*/
	this._init = function _init( options ) {
		var opts = {};
		var err;
		if ( options ) {
			err = validate( opts, options );
			if ( err ) {
				throw err;
			}
		}
		// Initialize variables:
		self.weights = null;
		self.lambda = opts.lambda || 1e-3;
		self.epsilon = opts.epsilon || 0.1;
		self.intercept = opts.intercept !== void 0 ? opts.intercept : true;

		// Set loss function:
		opts.loss = opts.loss || 'epsilonInsensitive';
		switch ( opts.loss ) {
		case 'epsilonInsensitive':
			self.lossfun = self.epsilonInsensitiveLoss;
		break;
		case 'huber':
			self.lossfun = self.huberLoss;
		break;
		case 'leastSquares':
			self.lossfun = self.leastSquaresLoss;
		break;
		default:
			throw Error( 'invalid input value. `loss` option must be either `epsilonInsensitive`, `huber` or `leastSquares`. Value: `' + opts.loss + '`' );
		}

		// Set learning rate:
		opts.learningRate = opts.learningRate || 'pegasos';
		switch ( opts.learningRate ) {
		case 'basic':
			self.getEta = function getEta() {
				return 10.0 / ( self.it + 10.0 );
			};
		break;
		case 'constant':
			self.getEta = function getEta() {
				return self.eta0;
			};
		break;
		case 'pegasos':
			// Default case: 'pegasos'
			self.getEta = function getEta() {
				return 1.0 / ( self.lambda * self.it );
			};
		break;
		default:
			throw Error( 'invalid input value. `learningRate` option must be either `basic`, `constant` or `pegasos`. Value: `' + opts.learningRate + '`' );
		}

		self.it = 1;
	}; // end METHOD _init()

	Object.defineProperty( this, 'coefs', {
		get: function getCoefs() {
			var ret;
			var i;

			ret = new Array( self.weights.nWeights );
			for ( i = 0; i < ret.length; i++ ) {
				ret[ i ] = self.weights._data[ i ] * self.weights.scale;
			}
			return ret;
		}
	});

	/**
	* Update weights given new observations `y` and `x` for a model without an intercept (bias) term.
	*
	* @private
	* @param {number} y - response value
	* @param {(Array|TypedArray)} x - feature vector
	*/
	this._updateWithoutIntercept = function update( y, x ) {
		var nWeights = self.weights.nWeights;
		var eta;

		if ( !isArray( x ) || x.length !== nWeights ) {
			throw new TypeError( 'invalid input value. Second argument `x` must be an array of length ' + nWeights + '. Value: `' + x + '`' );
		}

		// Get current learning rate...
		eta = self.getEta();

		// Perform L2 regularization...
		self.regularize( eta );

		// Update weights depending on the chosen loss function...
		self.lossfun( y, x, eta );

		// Perform projection...
		self.project();

		// Increase iteration counter...
		self.it += 1;
	}; // end METHOD _updateWithoutIntercept()

	/**
	* Update weights given new observations `y` and `x` for a model with an intercept (bias) term.
	*
	* @private
	* @param {number} y - response value
	* @param {(Array|TypedArray)} x - feature vector
	*/
	this._updateWithIntercept = function update( y, x ) {
		var nFeatures = self.weights.nWeights - 1;
		var eta;

		if ( !isArray( x ) || x.length !== nFeatures  ) {
			throw new TypeError( 'invalid input value. Second argument `x` must be an array of length ' + nFeatures + '. Value: `' + x + '`' );
		}

		// Get current learning rate...
		eta = self.getEta();

		// Perform L2 regularization...
		self.regularize( eta );

		// Update weights depending on the chosen loss function...
		self.lossfun( y, x, eta );

		// Perform projection...
		self.project();

		// Increase iteration counter...
		self.it += 1;
	}; // end METHOD _updateWithIntercept()

	/**
	* Function initially called when `.update()` is invoked. Creates an appropriately sized weight vector
	* and then overwrites the `.update()` reference depending on whether the created model should contain
	* an intercept or not.
	*
	* @private
	* @param {number} y - response value
	* @param {(Array|TypedArray)} x - feature vector
	*/
	this.update = function update( y, x ) {
		self.weights = new WeightVector( x.length, this.intercept );

		// Overwrite reference to update function:
		self.update = self.intercept ? self._updateWithIntercept : self._updateWithoutIntercept;

		// Call overwritten method:
		self.update( y, x );
	}; // end METHOD update()

	/**
	* Project weight vector onto the ball of radius 1 / sqrt(lambda).
	*
	* @private
	*/
	this.project = function project() {
		var proj  = 1.0 / sqrt( self.lambda * self.weights.norm );
		if ( proj < 1.0 ) {
			self.weights.scaleTo( proj );
		}
	}; // end METHOD project()

	/**
	* Predict response for a new observation with features `x`.
	*
	* @param {(Array|TypedArray)} x - feature vector
	* @returns {number} response value
	*/
	this.predict = function predict( x ) {
		return self.weights.innerProduct( x );
	}; // end METHOD predict()


	/**
	* Regularize feature weights.
	*
	* @private
	* @param {PositiveNumber} eta - current learning rate
	*/
	this.regularize = function regularize( eta ) {
		var scalingFactor = 1.0 - ( eta * self.lambda );
		self.weights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );
	}; // end METHOD regularize()

	// LOSS FUNCTIONS //

	/**
	* Update weights given new observation (y,x) when the used
	* loss function is the epsilon-insensitive loss (penalty is the absolute value
	* of the dot product of the weights and `x` minus `y` whenever the absolute error exceeds epsilon,
	* and zero otherwise).
	*
	* @private
	* @param {number} y - response value
	* @param {(Array|TypedArray)} x - feature vector
	* @param {PositiveNumber} eta - current learning rate
	*/
	this.epsilonInsensitiveLoss = function epsilonInsensitiveLoss( y, x, eta ) {
		var p = self.weights.innerProduct( x ) - y;
		if ( p > self.epsilon ) {
			self.weights.add( x, -eta );
		} else if ( p < -self.epsilon ) {
			self.weights.add( x, +eta );
		}
	}; // end METHOD epsilonInsensitiveLoss()

	/**
	* Update weights given new obersvation (y,x) when the used loss function is the Huber loss,
	* which uses squared-error loss for observations with error smaller than epsilon in magnitude and
	* and linear loss above that in order to decrease the influence of outliers on the model fit.
	*
	* @private
	* @param {number} y - response value
	* @param {(Array|TypedArray)} x - feature vector
	* @param {PositiveNumber} eta - current learning rate
	*/
	this.huberLoss = function huberLoss( y, x, eta ) {
		var p = self.weights.innerProduct( x ) - y;
		if ( p > self.epsilon ) {
			self.weights.add( x, -eta );
		} else if ( p < -self.epsilon ) {
			self.weights.add( x, +eta );
		} else {
			self.weights.add( x, -eta * p );
		}
	}; // end METHOD huberLoss()

	/**
	* Update weights given new obersvation (y,x) when the used loss function is the squared error loss,
	* i.e. the squared difference from the observed and fitted value.
	*
	* @private
	* @param {number} y - response value
	* @param {(Array|TypedArray)} x - feature vector
	* @param {PositiveNumber} eta - current learning rate
	*/
	this.leastSquaresLoss = function leastSquaresLoss( y, x, eta ) {
		var loss = y - self.weights.innerProduct( x );
		self.weights.add( x, ( eta * loss ) );
	}; // end METHOD leastSquaresLoss()

	// Initialize instance:
	self._init( options );
} // end FUNCTION OnlineSGDRegression()


// EXPORTS //

module.exports = OnlineSGDRegression;
