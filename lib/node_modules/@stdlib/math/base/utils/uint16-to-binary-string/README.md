# Binary String

> Return a string giving the literal bit representation of an [unsigned 16-bit integer][integer].


<section class="usage">

## Usage

``` javascript
var toBinaryString = require( '@stdlib/math/base/utils/uint16-to-binary-string' );
```

#### toBinaryString( x )

Returns a `string` giving the literal bit representation of an [unsigned 16-bit integer][integer].

``` javascript
var a = new Uint16Array( [ 1, 4, 9 ] );

var str = toBinaryString( a[0] );
// returns '0000000000000001'

str = toBinaryString( a[1] );
// returns '0000000000000100'

str = toBinaryString( a[2] );
// returns '0000000000001001'
```

</section>

<!-- /.usage -->


<section class="notes">

## Notes

* Except for [typed arrays][typed-arrays], JavaScript does __not__ provide native user support for [unsigned 16-bit integers][integer]. According to the [ECMAScript standard][ecma-262], `number` values correspond to [double-precision floating-point numbers][ieee754]. While this function is intended for [unsigned 16-bit integers][integer], the function will accept [floating-point][ieee754] values and represent the values __as if__ they are [unsigned 16-bit integers][integer]. Accordingly, care __should__ be taken to ensure that __only__ nonnegative integer values less than `65536` (`2^16`) are provided.

  ``` javascript
  var str = toBinaryString( 1 );
  // returns '0000000000000001'

  str = toBinaryString( 4 );
  // returns '0000000000000100'

  str = toBinaryString( 9 );
  // returns '0000000000001001'

  str = toBinaryString( 65535 );
  // returns '1111111111111111'
  ```

</section>

<!-- /.notes -->


<section class="examples">

## Examples

``` javascript
var randu = require( '@stdlib/math/base/random/randu' );
var round = require( '@stdlib/math/base/special/round' );
var MAX_UINT16 = require( '@stdlib/math/constants/uint16-max' );
var toBinaryString = require( '@stdlib/math/base/utils/uint16-to-binary-string' );

var x;
var y;
var b;
var i;

// Generate random unsigned 16-bit integers...
x = new Uint16Array( 100 );
for ( i = 0; i < x.length; i++ ) {
    x[ i ] = round( randu()*MAX_UINT16 );
}

// Convert unsigned 16-bit integers to literal bit representations...
for ( i = 0; i < x.length; i++ ) {
    b = bits( x[i] );
    y = parseInt( b, 2 );
    console.log( 'x: %d, b: %s, y: %d', x[i], b, y );
}
```

</section>

<!-- /.examples -->


<section class="links">

[integer]: https://en.wikipedia.org/wiki/Integer_%28computer_science%29
[typed-arrays]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Typed_arrays
[ecma-262]: http://www.ecma-international.org/ecma-262/5.1/#sec-4.3.19
[ieee754]: https://en.wikipedia.org/wiki/IEEE_754-1985

</section>

<!-- /.links -->
